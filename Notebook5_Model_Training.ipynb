{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Model with ML Classifiers\n",
    "\n",
    "This script performs **feature scaling, class balancing (SMOTE), model evaluation, and hyperparameter tuning** for diabetes prediction. It evaluates multiple classifiers (Logistic Regression, Decision Tree, KNN, Naive Bayes, Neural Network) and tunes **Random Forest, SVM, and XGBoost** for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1y5u0bl9q4j",
    "outputId": "5496c748-fe56-4606-a0ec-f819f8b27ce0"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Proportion:\n",
      " Diabetic\n",
      "0    0.762663\n",
      "2    0.206338\n",
      "1    0.025479\n",
      "3    0.005519\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Non-Tuned Models:\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Train Accuracy: 0.4975\n",
      "Validation Accuracy: 0.5020\n",
      "Test Accuracy: 0.5001\n",
      "Overfitting Check: No significant overfitting: Train-Val Accuracy Difference = -0.0045\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.33      0.41      6704\n",
      "           1       0.39      0.22      0.28      6752\n",
      "           2       0.48      0.59      0.53      6792\n",
      "           3       0.54      0.86      0.66      6782\n",
      "\n",
      "    accuracy                           0.50     27030\n",
      "   macro avg       0.49      0.50      0.47     27030\n",
      "weighted avg       0.49      0.50      0.47     27030\n",
      "\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "Train Accuracy: 0.6632\n",
      "Validation Accuracy: 0.6515\n",
      "Test Accuracy: 0.6542\n",
      "Overfitting Check: No significant overfitting: Train-Val Accuracy Difference = 0.0117\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65      6704\n",
      "           1       0.73      0.45      0.55      6752\n",
      "           2       0.54      0.61      0.57      6792\n",
      "           3       0.74      0.89      0.81      6782\n",
      "\n",
      "    accuracy                           0.65     27030\n",
      "   macro avg       0.66      0.65      0.65     27030\n",
      "weighted avg       0.66      0.65      0.65     27030\n",
      "\n",
      "\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Train Accuracy: 0.8972\n",
      "Validation Accuracy: 0.8501\n",
      "Test Accuracy: 0.8496\n",
      "Overfitting Check: No significant overfitting: Train-Val Accuracy Difference = 0.0471\n",
      "Classification Report for K-Nearest Neighbors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.57      0.68      6704\n",
      "           1       0.85      0.98      0.91      6752\n",
      "           2       0.76      0.85      0.80      6792\n",
      "           3       0.95      1.00      0.97      6782\n",
      "\n",
      "    accuracy                           0.85     27030\n",
      "   macro avg       0.85      0.85      0.84     27030\n",
      "weighted avg       0.85      0.85      0.84     27030\n",
      "\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "Train Accuracy: 0.4573\n",
      "Validation Accuracy: 0.4656\n",
      "Test Accuracy: 0.4579\n",
      "Overfitting Check: No significant overfitting: Train-Val Accuracy Difference = -0.0083\n",
      "Classification Report for Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.33      0.39      6704\n",
      "           1       0.42      0.12      0.18      6752\n",
      "           2       0.50      0.40      0.44      6792\n",
      "           3       0.44      0.98      0.61      6782\n",
      "\n",
      "    accuracy                           0.46     27030\n",
      "   macro avg       0.46      0.46      0.41     27030\n",
      "weighted avg       0.46      0.46      0.41     27030\n",
      "\n",
      "\n",
      "Evaluating Neural Network...\n",
      "Train Accuracy: 0.7303\n",
      "Validation Accuracy: 0.7254\n",
      "Test Accuracy: 0.7218\n",
      "Overfitting Check: No significant overfitting: Train-Val Accuracy Difference = 0.0049\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.52      0.58      6704\n",
      "           1       0.70      0.70      0.70      6752\n",
      "           2       0.63      0.68      0.65      6792\n",
      "           3       0.87      0.98      0.93      6782\n",
      "\n",
      "    accuracy                           0.72     27030\n",
      "   macro avg       0.72      0.72      0.72     27030\n",
      "weighted avg       0.72      0.72      0.72     27030\n",
      "\n",
      "\n",
      "Tuning Models:\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best Parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'class_weight': 'balanced'}\n",
      "Train Accuracy: 0.9995\n",
      "Validation Accuracy: 0.9119\n",
      "Test Accuracy: 0.9126\n",
      "Overfitting Check: Overfitting detected: Train-Val Accuracy Difference = 0.0876 > Threshold = 0.05\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      6704\n",
      "           1       0.96      0.96      0.96      6752\n",
      "           2       0.84      0.85      0.85      6792\n",
      "           3       0.99      0.99      0.99      6782\n",
      "\n",
      "    accuracy                           0.91     27030\n",
      "   macro avg       0.91      0.91      0.91     27030\n",
      "weighted avg       0.91      0.91      0.91     27030\n",
      "\n",
      "\n",
      "Tuning SVM (RBF Kernel)...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=20. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Updated imports with consistent organization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/wins_encoded_data.csv')\n",
    "\n",
    "# Display class distribution to compute class imbalance ratio\n",
    "class_proportion = df['Diabetic'].value_counts(normalize=True)\n",
    "print(\"Class Proportion:\\n\", class_proportion)\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop('Diabetic', axis=1)  # Features\n",
    "y = df['Diabetic']  # Target variable\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Split resampled data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to check for overfitting\n",
    "def check_overfitting(train_acc, val_acc, threshold=0.05):\n",
    "    diff = train_acc - val_acc\n",
    "    if diff > threshold:\n",
    "        return f\"Overfitting detected: Train-Val Accuracy Difference = {diff:.4f} > Threshold = {threshold}\"\n",
    "    else:\n",
    "        return f\"No significant overfitting: Train-Val Accuracy Difference = {diff:.4f}\"\n",
    "\n",
    "# Evaluate non-tuned models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Neural Network\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),\n",
    "        max_iter=300,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\\nEvaluating Non-Tuned Models:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on train, validation, and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Overfitting Check: {check_overfitting(train_accuracy, val_accuracy)}\")\n",
    "    print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, y_test_pred)}\")\n",
    "\n",
    "# Hyperparameter tuning for advanced models\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    \"SVM (RBF Kernel)\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'scale_pos_weight': [1, 2, 3]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nTuning Models:\")\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    if model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    elif model_name == \"SVM (RBF Kernel)\":\n",
    "        model = SVC(kernel='rbf', random_state=42)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate tuned model\n",
    "    y_train_pred = search.best_estimator_.predict(X_train)\n",
    "    y_val_pred = search.best_estimator_.predict(X_val)\n",
    "    y_test_pred = search.best_estimator_.predict(X_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Best Parameters for {model_name}: {search.best_params_}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Overfitting Check: {check_overfitting(train_accuracy, val_accuracy)}\")\n",
    "    print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, y_test_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The best performing model is **Random Forest (Tuned)** with a test accuracy of **91.26%**, though it suffers from overfitting. **K-Nearest Neighbors** performed well without overfitting, achieving a test accuracy of **84.96%**. **Naive Bayes** had the lowest performance with **45.79%** accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
